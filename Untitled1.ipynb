{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "mount_file_id": "1mMgvVAkJPuerod7wuXtahRRAnHy74jrf",
      "authorship_tag": "ABX9TyO05i/kgERiA0f4pHs5szNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladislav805/baccara-neural/blob/v3/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSW9-pKZMkg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "231e781a-a974-4869-cc66-d46de2448c2f"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Tuple, Union, List, Optional\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from time import time\n",
        "\n",
        "import sys\n",
        "\n",
        "\n",
        "import os.path\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "def do_bot(_):\n",
        "    # Вычисляем коэффицинты нормализации из обучающего датасета\n",
        "    # mean, sd = get_normalization_params()\n",
        "\n",
        "    # Создаём обработчик для бота\n",
        "    bot_handler = bot_factory()\n",
        "\n",
        "    bot_handler()\n",
        "    set_interval(bot_handler, 30)\n",
        "\n",
        "\n",
        "def bot_factory():\n",
        "    # Создаём и загружаем модель\n",
        "    model = get_model(path=MODEL_TRAINED_PATH)\n",
        "\n",
        "    def handler():\n",
        "        # Получаем предыдущие игры\n",
        "        last_signals, last_signal_id = get_last_signals()\n",
        "\n",
        "        if len(last_signals) == 0 and last_signal_id is None:\n",
        "            print('not enough data')\n",
        "            return\n",
        "\n",
        "        # Предсказываем\n",
        "        (predict_orig, predict_norm) = predict_by_last_signals(model=model,\n",
        "                                                               dataset=last_signals,\n",
        "                                                               # mean=mean,\n",
        "                                                               # sd=sd\n",
        "                                                               )\n",
        "\n",
        "        card = get_card_by_id(predict_norm)\n",
        "\n",
        "        text = \"Signal = {0}\\nPredict = {1}\\nNormalized predict = {2}\\nCard = {3}\".format(\n",
        "            last_signal_id + 1,\n",
        "            predict_orig,\n",
        "            predict_norm,\n",
        "            card,\n",
        "        )\n",
        "\n",
        "        for chat_id in BOT_TARGET_IDS:\n",
        "            send_telegram_message(chat_id=chat_id, text=text)\n",
        "\n",
        "    return handler\n",
        "\n",
        "\n",
        "def send_telegram_message(chat_id: int, text: str):\n",
        "    token = os.getenv('TG_TOKEN')\n",
        "\n",
        "    if token is None:\n",
        "        print('TG_TOKEN not set')\n",
        "        exit(1)\n",
        "\n",
        "    requests.get('https://api.telegram.org/bot' + token + '/sendMessage',\n",
        "                 params={\n",
        "                     'chat_id': chat_id,\n",
        "                     'text': text\n",
        "                 }).json()\n",
        "\n",
        "\n",
        "\n",
        "# Путь exit(0)(название директории) к уже обученной модели\n",
        "MODEL_TRAINED_PATH = 'trained'\n",
        "\n",
        "# Индекс параметра, который будем предсказывать\n",
        "DATASET_COLUMN_INDEX = 0\n",
        "\n",
        "# Количество строк, по которым будем пытаться предсказывать\n",
        "HISTORY_SIZE = 1\n",
        "\n",
        "# Сколько вперед будем предсказывать\n",
        "TARGET_SIZE = 1\n",
        "\n",
        "# Куда слать сообщения\n",
        "BOT_TARGET_IDS = [63923, 485056]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(dataset: pd.Series, by: int):\n",
        "    return dataset[:by], dataset[by:]\n",
        "\n",
        "\n",
        "# нет поддержки single_step = False\n",
        "def multivariate_dataset(dataset,  # наш датасет (матрица)\n",
        "                         column_index: int,  # какой столбец будем обучать\n",
        "                         history_size: int,  # сколько строк в прошлом будем брать\n",
        "                         target_size: int,  # сколько вперед считаем\n",
        "                         ):\n",
        "    xs = []  # параметры\n",
        "    ys = []  # ответы\n",
        "\n",
        "    # последняя итерация на элементе, индекс которого:\n",
        "    end_index = len(dataset) - target_size + 1\n",
        "\n",
        "    for i in range(history_size, end_index):\n",
        "        start = i - history_size\n",
        "        end = i\n",
        "\n",
        "        # берем строки с индексом от `i` до `i + history_size`\n",
        "        indexes = range(start, end)\n",
        "\n",
        "        # добавляем их в массив с параметрами\n",
        "        xs.append(normalize_dataset(dataset[indexes]))\n",
        "\n",
        "        # добавляем ответ из строки `текущая + target`\n",
        "        ys.append(card_to_vector(dataset[end][column_index]))\n",
        "\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "\n",
        "# Названия колонок, которые будут использоваться как параметры\n",
        "\n",
        "features_considered = [\n",
        "    'cardPlayer1', 'cardPlayer2', 'mastPlayer1', 'mastPlayer2',\n",
        "    'cardBanker1', 'cardBanker2', 'mastBanker1', 'mastBanker2',\n",
        "]\n",
        "\n",
        "\n",
        "def get_last_signal_id(dataset) -> int:\n",
        "    return int(dataset[-1:]['signal_id'])\n",
        "\n",
        "\n",
        "# Получить датасет, по которому будем обучать\n",
        "def get_train_data() -> Tuple[pd.Series, int]:\n",
        "    zip_path = tf.keras.utils.get_file(\n",
        "        # origin='http://longpoll.ru/dev/export.php?since=51587&count=54158',\n",
        "        origin='http://185.93.111.205/zalupa_na_provode.csv',\n",
        "        fname='zalupa_na_provode')\n",
        "\n",
        "    csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    last_signal_id = get_last_signal_id(df)\n",
        "\n",
        "    # Получение только выбранных колонок (параметров)\n",
        "    features = df[features_considered]\n",
        "    features.index = df['id']  # добавление индекса\n",
        "\n",
        "    return features.values, last_signal_id\n",
        "\n",
        "\n",
        "# Получить последние сигналы\n",
        "def get_last_signals() -> Tuple[Union[pd.Series, list], Optional[int]]:\n",
        "    try:\n",
        "        ncrnd = str(time())\n",
        "        path = tf.keras.utils.get_file(\n",
        "            'last_games' + ncrnd,\n",
        "            origin='http://185.93.111.205/dev/export.php?' + ncrnd,\n",
        "        )\n",
        "\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        last_signal_id = get_last_signal_id(df)\n",
        "\n",
        "        last_games = df[features_considered]\n",
        "        last_games.index = df['id']\n",
        "\n",
        "        return last_games.values, last_signal_id\n",
        "    except Exception:\n",
        "        return [], None\n",
        "\n",
        "\n",
        "def get_signals_in_range(start, end) -> Tuple[Union[pd.Series, list], Optional[int]]:\n",
        "    try:\n",
        "        path = tf.keras.utils.get_file(\n",
        "            'games_since_{0}_{1}'.format(start, end),\n",
        "            origin='http://52.18.14.99/dev/export.php?since={0}&count={1}'.format(start, end - start),\n",
        "        )\n",
        "\n",
        "        data = pd.read_csv(path)\n",
        "\n",
        "        last_signal_id = get_last_signal_id(data)\n",
        "\n",
        "        games = data[features_considered]\n",
        "        games.index = data['id']\n",
        "\n",
        "        return games.values, last_signal_id\n",
        "    except Exception:\n",
        "        return [], None\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "\n",
        "\n",
        "def get_model(path: str = None,  # Путь к модели\n",
        "              input_shape: Optional[Tuple[int, int]] = None,  # Размерность входа\n",
        "              train: bool = False,  # Нужно ли её обучать или просто прочитать?\n",
        "              ) -> tf.keras.models.Model:\n",
        "    exists = os.path.exists(path) if path is not None else False\n",
        "    if not train:\n",
        "        if exists:\n",
        "            return tf.keras.models.load_model(path)\n",
        "        else:\n",
        "            raise FileNotFoundError('Trained model on path {} not found'.format(path))\n",
        "    else:\n",
        "        if exists:\n",
        "            answer = input('Model on path {} already exists. Rewrite? (y - continue): ')\n",
        "            if answer != 'y' and answer != 'н':\n",
        "                exit(0)\n",
        "\n",
        "        if path is not None:\n",
        "            return create_model(input_shape=input_shape)\n",
        "        else:\n",
        "            raise FileNotFoundError('path not specified')\n",
        "\n",
        "\n",
        "# Просто создаём модель\n",
        "def create_model(input_shape):\n",
        "\n",
        "    print(input_shape)\n",
        "    # Создание модели\n",
        "    model = tf.keras.models.Sequential()\n",
        "    # , activation='softmax, dropout=0.1'\n",
        "    model.add(tf.keras.layers.LSTM(40, input_shape=input_shape, activation=\"tanh\"))\n",
        "    # model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(200, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(13, activation='softmax'))\n",
        "    # model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    # Компиляция модели learning_rate=0.0000005\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.000005), loss='categorical_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_dataset(dataset: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
        "\n",
        "    # Нормализация данных\n",
        "    dataset = tf.keras.utils.normalize(dataset, axis=-1, order=2)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def normalize_value(value: float, mean: float, sd: float) -> float:\n",
        "    return value * sd + mean\n",
        "\n",
        "\n",
        "SINCE = 147141\n",
        "\n",
        "\n",
        "def predict_by_last_signals(model,\n",
        "                            dataset,\n",
        "                            #mean: List[float],\n",
        "                            #sd: List[float]\n",
        "                            ):\n",
        "    last_signals = dataset\n",
        "    #last_signals, _, _ = normalize_dataset(dataset, mean, sd)\n",
        "\n",
        "    # Формируем датасет\n",
        "    dataset = tf.data.Dataset.from_tensors(last_signals) \\\n",
        "        .batch(1) \\\n",
        "        .repeat()\n",
        "\n",
        "    # Берем из датасета одну матрицу\n",
        "    item = dataset.take(1)\n",
        "\n",
        "    if model is None:\n",
        "        model = get_model(MODEL_TRAINED_PATH)\n",
        "\n",
        "    predict = vector_to_card(model.predict(item)[0])\n",
        "\n",
        "    # Предсказываем\n",
        "    \"\"\"\n",
        "        predicted = normalize_value(\n",
        "            predict,\n",
        "            mean[DATASET_COLUMN_INDEX],\n",
        "            sd[DATASET_COLUMN_INDEX])\n",
        "    \"\"\"\n",
        "    return predict, int(np.fix(predict))\n",
        "\n",
        "\n",
        "def do_predict(_):\n",
        "    real = None\n",
        "\n",
        "    # Если не укзано с какого сигнала брать (для тестов), то берем последние\n",
        "    # Иначе берем от SINCE до SINCE + 720\n",
        "    if SINCE is None:\n",
        "        last_signals, last_signal_id = get_last_signals()\n",
        "    else:\n",
        "        last_signals, last_signal_id = get_signals_in_range(SINCE, SINCE + HISTORY_SIZE + 1)\n",
        "        last_signals, real = split_dataset(last_signals, by=HISTORY_SIZE)\n",
        "\n",
        "    # Получаем номер следущего сигнала\n",
        "    target_signal_id = last_signal_id + 1\n",
        "\n",
        "    # Вычисляем коэффицинты нормализации из обучающего датасета\n",
        "    #mean, sd = get_normalization_params()\n",
        "\n",
        "    # Создаём и загружаем модель\n",
        "    model = get_model(path=MODEL_TRAINED_PATH)\n",
        "\n",
        "    # Предсказываем\n",
        "    predicted = predict_by_last_signals(model, last_signals)\n",
        "\n",
        "    if SINCE is not None:\n",
        "        print('predicted =', predicted, 'real =', real[0][DATASET_COLUMN_INDEX])\n",
        "    else:\n",
        "        print('in id = ', target_signal_id, 'predicted = ', predicted)\n",
        "\n",
        "\n",
        "MASS_TEST_SINCE = 147141\n",
        "MASS_TEST_ITERATIONS = 250\n",
        "\n",
        "\n",
        "def do_mass_test(_):\n",
        "    # mean, sd = get_normalization_params()\n",
        "    passed = 0\n",
        "\n",
        "    # Создаём и загружаем модель\n",
        "    model = get_model(path=MODEL_TRAINED_PATH)\n",
        "\n",
        "    for row_id in range(MASS_TEST_SINCE, MASS_TEST_SINCE + MASS_TEST_ITERATIONS):\n",
        "        last_signals, last_signal_id = get_signals_in_range(row_id, row_id + HISTORY_SIZE + 1)\n",
        "        last_signals, real = split_dataset(last_signals, by=HISTORY_SIZE)\n",
        "\n",
        "        predicted, predicted_value = predict_by_last_signals(model=model,\n",
        "                                                             dataset=last_signals,\n",
        "                                                             # mean=mean,\n",
        "                                                             # sd=sd\n",
        "                                                             )\n",
        "        val = real[0][DATASET_COLUMN_INDEX]\n",
        "        print('predicted =', predicted, 'value =', predicted_value, 'real =', val)\n",
        "        if val == predicted_value:\n",
        "            passed = passed + 1\n",
        "\n",
        "    print('passed ', passed, 'of', MASS_TEST_ITERATIONS, ', ', round(passed * 100 / MASS_TEST_ITERATIONS, 2), '%')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "DATASET_TRAIN_SPLIT = 24991\n",
        "\n",
        "# tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "\n",
        "def get_normalization_params():\n",
        "    dataset, _ = get_train_data()\n",
        "    train, _ = split_dataset(dataset, DATASET_TRAIN_SPLIT)\n",
        "    _, mean, sd = normalize_dataset(train)\n",
        "    return mean, sd\n",
        "\n",
        "\n",
        "def do_train(_: None):\n",
        "    # скачали и подготовили датасет\n",
        "    dataset, _ = get_train_data()\n",
        "\n",
        "    train, validate = split_dataset(dataset, DATASET_TRAIN_SPLIT)\n",
        "\n",
        "    # нормализовали данные для обучения\n",
        "    # train = normalize_dataset(train)\n",
        "\n",
        "    # разбили данные для обучения\n",
        "    train_x, train_y = multivariate_dataset(\n",
        "        dataset=train,\n",
        "        column_index=DATASET_COLUMN_INDEX,\n",
        "        history_size=HISTORY_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "    )\n",
        "\n",
        "    # validate = normalize_dataset(validate)\n",
        "\n",
        "    # разбили данные для валидации\n",
        "    validate_x, validate_y = multivariate_dataset(\n",
        "        dataset=validate,\n",
        "        column_index=DATASET_COLUMN_INDEX,\n",
        "        history_size=HISTORY_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "    )\n",
        "\n",
        "    batch = 1\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\\\n",
        "        .shuffle(1440)\\\n",
        "        .batch(batch) \\\n",
        "        .cache()\\\n",
        "        # .repeat()\n",
        "\n",
        "    validate_dataset = tf.data.Dataset.from_tensor_slices((validate_x, validate_y))\\\n",
        "        .batch(batch)\\\n",
        "        # .repeat()\n",
        "\n",
        "    print(validate_y)\n",
        "\n",
        "    model = get_model(path=MODEL_TRAINED_PATH,\n",
        "                      input_shape=(1, 8),\n",
        "                      train=True)\n",
        "\n",
        "    model_history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=validate_dataset,\n",
        "        validation_steps=1440,\n",
        "        epochs=30,\n",
        "        # steps_per_epoch=19280,\n",
        "        # initial_epoch=10,\n",
        "        batch_size=1,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                filepath=MODEL_TRAINED_PATH,\n",
        "                save_weights_only=False,\n",
        "                save_best_only=True,\n",
        "            )\n",
        "        ],\n",
        "        use_multiprocessing=True,\n",
        "    )\n",
        "\n",
        "    loss = model_history.history['loss']\n",
        "    val_loss = model_history.history['val_loss']\n",
        "    epochs = range(len(loss))\n",
        "    plt.figure(1)\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import threading\n",
        "\n",
        "def set_interval(func, sec):\n",
        "    def func_wrapper():\n",
        "        set_interval(func, sec)\n",
        "        func()\n",
        "    t = threading.Timer(sec, func_wrapper)\n",
        "    t.start()\n",
        "    return t\n",
        "\n",
        "\n",
        "__cards = {\n",
        "    1: '1️⃣',\n",
        "    2: '2️⃣',\n",
        "    3: '3️⃣',\n",
        "    4: '4️⃣',\n",
        "    5: '5️⃣',\n",
        "    6: '6️⃣',\n",
        "    7: '7️⃣',\n",
        "    8: '8️⃣',\n",
        "    9: '9️⃣',\n",
        "    10: '🔟',\n",
        "    11: \"J\",\n",
        "    12: \"Q\",\n",
        "    13: \"K\",\n",
        "    14: \"A\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_card_by_id(id: float) -> str:\n",
        "    i = int(id)\n",
        "    return 'UNKNOWN' if i not in __cards else __cards[i]\n",
        "\n",
        "\n",
        "def card_to_vector(n: int) -> List[int]:\n",
        "    vec = [0] * 13\n",
        "    vec[n - 2] = 1\n",
        "    return vec\n",
        "\n",
        "\n",
        "def vector_to_card(vec) -> int:\n",
        "    return numpy.argmax(vec) + 2\n",
        "\n",
        "\n",
        "\n",
        "commands = {\n",
        "    'train': do_train,\n",
        "    'predict': do_predict,\n",
        "    'bot': do_bot,\n",
        "    'mass_test': do_mass_test,\n",
        "}\n",
        "\n",
        "\n",
        "do_mass_test(_)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-45f381eaff87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m \u001b[0mdo_mass_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-45f381eaff87>\u001b[0m in \u001b[0;36mdo_mass_test\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# Создаём и загружаем модель\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_TRAINED_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMASS_TEST_SINCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASS_TEST_SINCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMASS_TEST_ITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-45f381eaff87>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(path, input_shape, train)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trained model on path {} not found'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Trained model on path trained not found"
          ]
        }
      ]
    }
  ]
}