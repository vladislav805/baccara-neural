{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "mount_file_id": "1mMgvVAkJPuerod7wuXtahRRAnHy74jrf",
      "authorship_tag": "ABX9TyO05i/kgERiA0f4pHs5szNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladislav805/baccara-neural/blob/v3/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSW9-pKZMkg6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "231e781a-a974-4869-cc66-d46de2448c2f"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Tuple, Union, List, Optional\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from time import time\n",
        "\n",
        "import sys\n",
        "\n",
        "\n",
        "import os.path\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "def do_bot(_):\n",
        "    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸Ð½Ñ‚Ñ‹ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸Ð· Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ³Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n",
        "    # mean, sd = get_normalization_params()\n",
        "\n",
        "    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð´Ð»Ñ Ð±Ð¾Ñ‚Ð°\n",
        "    bot_handler = bot_factory()\n",
        "\n",
        "    bot_handler()\n",
        "    set_interval(bot_handler, 30)\n",
        "\n",
        "\n",
        "def bot_factory():\n",
        "    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
        "    model = get_model(path=MODEL_TRAINED_PATH)\n",
        "\n",
        "    def handler():\n",
        "        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ Ð¸Ð³Ñ€Ñ‹\n",
        "        last_signals, last_signal_id = get_last_signals()\n",
        "\n",
        "        if len(last_signals) == 0 and last_signal_id is None:\n",
        "            print('not enough data')\n",
        "            return\n",
        "\n",
        "        # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼\n",
        "        (predict_orig, predict_norm) = predict_by_last_signals(model=model,\n",
        "                                                               dataset=last_signals,\n",
        "                                                               # mean=mean,\n",
        "                                                               # sd=sd\n",
        "                                                               )\n",
        "\n",
        "        card = get_card_by_id(predict_norm)\n",
        "\n",
        "        text = \"Signal = {0}\\nPredict = {1}\\nNormalized predict = {2}\\nCard = {3}\".format(\n",
        "            last_signal_id + 1,\n",
        "            predict_orig,\n",
        "            predict_norm,\n",
        "            card,\n",
        "        )\n",
        "\n",
        "        for chat_id in BOT_TARGET_IDS:\n",
        "            send_telegram_message(chat_id=chat_id, text=text)\n",
        "\n",
        "    return handler\n",
        "\n",
        "\n",
        "def send_telegram_message(chat_id: int, text: str):\n",
        "    token = os.getenv('TG_TOKEN')\n",
        "\n",
        "    if token is None:\n",
        "        print('TG_TOKEN not set')\n",
        "        exit(1)\n",
        "\n",
        "    requests.get('https://api.telegram.org/bot' + token + '/sendMessage',\n",
        "                 params={\n",
        "                     'chat_id': chat_id,\n",
        "                     'text': text\n",
        "                 }).json()\n",
        "\n",
        "\n",
        "\n",
        "# ÐŸÑƒÑ‚ÑŒ exit(0)(Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸) Ðº ÑƒÐ¶Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "MODEL_TRAINED_PATH = 'trained'\n",
        "\n",
        "# Ð˜Ð½Ð´ÐµÐºÑ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð±ÑƒÐ´ÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ\n",
        "DATASET_COLUMN_INDEX = 0\n",
        "\n",
        "# ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð¾Ðº, Ð¿Ð¾ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼ Ð±ÑƒÐ´ÐµÐ¼ Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ\n",
        "HISTORY_SIZE = 1\n",
        "\n",
        "# Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ Ð²Ð¿ÐµÑ€ÐµÐ´ Ð±ÑƒÐ´ÐµÐ¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ\n",
        "TARGET_SIZE = 1\n",
        "\n",
        "# ÐšÑƒÐ´Ð° ÑÐ»Ð°Ñ‚ÑŒ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ\n",
        "BOT_TARGET_IDS = [63923, 485056]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_dataset(dataset: pd.Series, by: int):\n",
        "    return dataset[:by], dataset[by:]\n",
        "\n",
        "\n",
        "# Ð½ÐµÑ‚ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸ single_step = False\n",
        "def multivariate_dataset(dataset,  # Ð½Ð°Ñˆ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ (Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°)\n",
        "                         column_index: int,  # ÐºÐ°ÐºÐ¾Ð¹ ÑÑ‚Ð¾Ð»Ð±ÐµÑ† Ð±ÑƒÐ´ÐµÐ¼ Ð¾Ð±ÑƒÑ‡Ð°Ñ‚ÑŒ\n",
        "                         history_size: int,  # ÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð¾Ðº Ð² Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¼ Ð±ÑƒÐ´ÐµÐ¼ Ð±Ñ€Ð°Ñ‚ÑŒ\n",
        "                         target_size: int,  # ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð²Ð¿ÐµÑ€ÐµÐ´ ÑÑ‡Ð¸Ñ‚Ð°ÐµÐ¼\n",
        "                         ):\n",
        "    xs = []  # Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹\n",
        "    ys = []  # Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹\n",
        "\n",
        "    # Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÑÑ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½Ð° ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ðµ, Ð¸Ð½Ð´ÐµÐºÑ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾:\n",
        "    end_index = len(dataset) - target_size + 1\n",
        "\n",
        "    for i in range(history_size, end_index):\n",
        "        start = i - history_size\n",
        "        end = i\n",
        "\n",
        "        # Ð±ÐµÑ€ÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð¼ Ð¾Ñ‚ `i` Ð´Ð¾ `i + history_size`\n",
        "        indexes = range(start, end)\n",
        "\n",
        "        # Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ñ… Ð² Ð¼Ð°ÑÑÐ¸Ð² Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸\n",
        "        xs.append(normalize_dataset(dataset[indexes]))\n",
        "\n",
        "        # Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¸Ð· ÑÑ‚Ñ€Ð¾ÐºÐ¸ `Ñ‚ÐµÐºÑƒÑ‰Ð°Ñ + target`\n",
        "        ys.append(card_to_vector(dataset[end][column_index]))\n",
        "\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "\n",
        "# ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð±ÑƒÐ´ÑƒÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ ÐºÐ°Ðº Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹\n",
        "\n",
        "features_considered = [\n",
        "    'cardPlayer1', 'cardPlayer2', 'mastPlayer1', 'mastPlayer2',\n",
        "    'cardBanker1', 'cardBanker2', 'mastBanker1', 'mastBanker2',\n",
        "]\n",
        "\n",
        "\n",
        "def get_last_signal_id(dataset) -> int:\n",
        "    return int(dataset[-1:]['signal_id'])\n",
        "\n",
        "\n",
        "# ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚, Ð¿Ð¾ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼Ñƒ Ð±ÑƒÐ´ÐµÐ¼ Ð¾Ð±ÑƒÑ‡Ð°Ñ‚ÑŒ\n",
        "def get_train_data() -> Tuple[pd.Series, int]:\n",
        "    zip_path = tf.keras.utils.get_file(\n",
        "        # origin='http://longpoll.ru/dev/export.php?since=51587&count=54158',\n",
        "        origin='http://185.93.111.205/zalupa_na_provode.csv',\n",
        "        fname='zalupa_na_provode')\n",
        "\n",
        "    csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    last_signal_id = get_last_signal_id(df)\n",
        "\n",
        "    # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº (Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²)\n",
        "    features = df[features_considered]\n",
        "    features.index = df['id']  # Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ°\n",
        "\n",
        "    return features.values, last_signal_id\n",
        "\n",
        "\n",
        "# ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¸Ð³Ð½Ð°Ð»Ñ‹\n",
        "def get_last_signals() -> Tuple[Union[pd.Series, list], Optional[int]]:\n",
        "    try:\n",
        "        ncrnd = str(time())\n",
        "        path = tf.keras.utils.get_file(\n",
        "            'last_games' + ncrnd,\n",
        "            origin='http://185.93.111.205/dev/export.php?' + ncrnd,\n",
        "        )\n",
        "\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        last_signal_id = get_last_signal_id(df)\n",
        "\n",
        "        last_games = df[features_considered]\n",
        "        last_games.index = df['id']\n",
        "\n",
        "        return last_games.values, last_signal_id\n",
        "    except Exception:\n",
        "        return [], None\n",
        "\n",
        "\n",
        "def get_signals_in_range(start, end) -> Tuple[Union[pd.Series, list], Optional[int]]:\n",
        "    try:\n",
        "        path = tf.keras.utils.get_file(\n",
        "            'games_since_{0}_{1}'.format(start, end),\n",
        "            origin='http://52.18.14.99/dev/export.php?since={0}&count={1}'.format(start, end - start),\n",
        "        )\n",
        "\n",
        "        data = pd.read_csv(path)\n",
        "\n",
        "        last_signal_id = get_last_signal_id(data)\n",
        "\n",
        "        games = data[features_considered]\n",
        "        games.index = data['id']\n",
        "\n",
        "        return games.values, last_signal_id\n",
        "    except Exception:\n",
        "        return [], None\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "\n",
        "\n",
        "def get_model(path: str = None,  # ÐŸÑƒÑ‚ÑŒ Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "              input_shape: Optional[Tuple[int, int]] = None,  # Ð Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ…Ð¾Ð´Ð°\n",
        "              train: bool = False,  # ÐÑƒÐ¶Ð½Ð¾ Ð»Ð¸ ÐµÑ‘ Ð¾Ð±ÑƒÑ‡Ð°Ñ‚ÑŒ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ?\n",
        "              ) -> tf.keras.models.Model:\n",
        "    exists = os.path.exists(path) if path is not None else False\n",
        "    if not train:\n",
        "        if exists:\n",
        "            return tf.keras.models.load_model(path)\n",
        "        else:\n",
        "            raise FileNotFoundError('Trained model on path {} not found'.format(path))\n",
        "    else:\n",
        "        if exists:\n",
        "            answer = input('Model on path {} already exists. Rewrite? (y - continue): ')\n",
        "            if answer != 'y' and answer != 'Ð½':\n",
        "                exit(0)\n",
        "\n",
        "        if path is not None:\n",
        "            return create_model(input_shape=input_shape)\n",
        "        else:\n",
        "            raise FileNotFoundError('path not specified')\n",
        "\n",
        "\n",
        "# ÐŸÑ€Ð¾ÑÑ‚Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
        "def create_model(input_shape):\n",
        "\n",
        "    print(input_shape)\n",
        "    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "    model = tf.keras.models.Sequential()\n",
        "    # , activation='softmax, dropout=0.1'\n",
        "    model.add(tf.keras.layers.LSTM(40, input_shape=input_shape, activation=\"tanh\"))\n",
        "    # model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dense(200, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(13, activation='softmax'))\n",
        "    # model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    # ÐšÐ¾Ð¼Ð¿Ð¸Ð»ÑÑ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ learning_rate=0.0000005\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.000005), loss='categorical_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalize_dataset(dataset: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
        "\n",
        "    # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
        "    dataset = tf.keras.utils.normalize(dataset, axis=-1, order=2)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def normalize_value(value: float, mean: float, sd: float) -> float:\n",
        "    return value * sd + mean\n",
        "\n",
        "\n",
        "SINCE = 147141\n",
        "\n",
        "\n",
        "def predict_by_last_signals(model,\n",
        "                            dataset,\n",
        "                            #mean: List[float],\n",
        "                            #sd: List[float]\n",
        "                            ):\n",
        "    last_signals = dataset\n",
        "    #last_signals, _, _ = normalize_dataset(dataset, mean, sd)\n",
        "\n",
        "    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚\n",
        "    dataset = tf.data.Dataset.from_tensors(last_signals) \\\n",
        "        .batch(1) \\\n",
        "        .repeat()\n",
        "\n",
        "    # Ð‘ÐµÑ€ÐµÐ¼ Ð¸Ð· Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° Ð¾Ð´Ð½Ñƒ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñƒ\n",
        "    item = dataset.take(1)\n",
        "\n",
        "    if model is None:\n",
        "        model = get_model(MODEL_TRAINED_PATH)\n",
        "\n",
        "    predict = vector_to_card(model.predict(item)[0])\n",
        "\n",
        "    # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼\n",
        "    \"\"\"\n",
        "        predicted = normalize_value(\n",
        "            predict,\n",
        "            mean[DATASET_COLUMN_INDEX],\n",
        "            sd[DATASET_COLUMN_INDEX])\n",
        "    \"\"\"\n",
        "    return predict, int(np.fix(predict))\n",
        "\n",
        "\n",
        "def do_predict(_):\n",
        "    real = None\n",
        "\n",
        "    # Ð•ÑÐ»Ð¸ Ð½Ðµ ÑƒÐºÐ·Ð°Ð½Ð¾ Ñ ÐºÐ°ÐºÐ¾Ð³Ð¾ ÑÐ¸Ð³Ð½Ð°Ð»Ð° Ð±Ñ€Ð°Ñ‚ÑŒ (Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²), Ñ‚Ð¾ Ð±ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ\n",
        "    # Ð˜Ð½Ð°Ñ‡Ðµ Ð±ÐµÑ€ÐµÐ¼ Ð¾Ñ‚ SINCE Ð´Ð¾ SINCE + 720\n",
        "    if SINCE is None:\n",
        "        last_signals, last_signal_id = get_last_signals()\n",
        "    else:\n",
        "        last_signals, last_signal_id = get_signals_in_range(SINCE, SINCE + HISTORY_SIZE + 1)\n",
        "        last_signals, real = split_dataset(last_signals, by=HISTORY_SIZE)\n",
        "\n",
        "    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð½Ð¾Ð¼ÐµÑ€ ÑÐ»ÐµÐ´ÑƒÑ‰ÐµÐ³Ð¾ ÑÐ¸Ð³Ð½Ð°Ð»Ð°\n",
        "    target_signal_id = last_signal_id + 1\n",
        "\n",
        "    # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÐºÐ¾ÑÑ„Ñ„Ð¸Ñ†Ð¸Ð½Ñ‚Ñ‹ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¸Ð· Ð¾Ð±ÑƒÑ‡Ð°ÑŽÑ‰ÐµÐ³Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°\n",
        "    #mean, sd = get_normalization_params()\n",
        "\n",
        "    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
        "    model = get_model(path=MODEL_TRAINED_PATH)\n",
        "\n",
        "    # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼\n",
        "    predicted = predict_by_last_signals(model, last_signals)\n",
        "\n",
        "    if SINCE is not None:\n",
        "        print('predicted =', predicted, 'real =', real[0][DATASET_COLUMN_INDEX])\n",
        "    else:\n",
        "        print('in id = ', target_signal_id, 'predicted = ', predicted)\n",
        "\n",
        "\n",
        "MASS_TEST_SINCE = 147141\n",
        "MASS_TEST_ITERATIONS = 250\n",
        "\n",
        "\n",
        "def do_mass_test(_):\n",
        "    # mean, sd = get_normalization_params()\n",
        "    passed = 0\n",
        "\n",
        "    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\n",
        "    model = get_model(path=MODEL_TRAINED_PATH)\n",
        "\n",
        "    for row_id in range(MASS_TEST_SINCE, MASS_TEST_SINCE + MASS_TEST_ITERATIONS):\n",
        "        last_signals, last_signal_id = get_signals_in_range(row_id, row_id + HISTORY_SIZE + 1)\n",
        "        last_signals, real = split_dataset(last_signals, by=HISTORY_SIZE)\n",
        "\n",
        "        predicted, predicted_value = predict_by_last_signals(model=model,\n",
        "                                                             dataset=last_signals,\n",
        "                                                             # mean=mean,\n",
        "                                                             # sd=sd\n",
        "                                                             )\n",
        "        val = real[0][DATASET_COLUMN_INDEX]\n",
        "        print('predicted =', predicted, 'value =', predicted_value, 'real =', val)\n",
        "        if val == predicted_value:\n",
        "            passed = passed + 1\n",
        "\n",
        "    print('passed ', passed, 'of', MASS_TEST_ITERATIONS, ', ', round(passed * 100 / MASS_TEST_ITERATIONS, 2), '%')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "DATASET_TRAIN_SPLIT = 24991\n",
        "\n",
        "# tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "\n",
        "def get_normalization_params():\n",
        "    dataset, _ = get_train_data()\n",
        "    train, _ = split_dataset(dataset, DATASET_TRAIN_SPLIT)\n",
        "    _, mean, sd = normalize_dataset(train)\n",
        "    return mean, sd\n",
        "\n",
        "\n",
        "def do_train(_: None):\n",
        "    # ÑÐºÐ°Ñ‡Ð°Ð»Ð¸ Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ð»Ð¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚\n",
        "    dataset, _ = get_train_data()\n",
        "\n",
        "    train, validate = split_dataset(dataset, DATASET_TRAIN_SPLIT)\n",
        "\n",
        "    # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
        "    # train = normalize_dataset(train)\n",
        "\n",
        "    # Ñ€Ð°Ð·Ð±Ð¸Ð»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ\n",
        "    train_x, train_y = multivariate_dataset(\n",
        "        dataset=train,\n",
        "        column_index=DATASET_COLUMN_INDEX,\n",
        "        history_size=HISTORY_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "    )\n",
        "\n",
        "    # validate = normalize_dataset(validate)\n",
        "\n",
        "    # Ñ€Ð°Ð·Ð±Ð¸Ð»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸\n",
        "    validate_x, validate_y = multivariate_dataset(\n",
        "        dataset=validate,\n",
        "        column_index=DATASET_COLUMN_INDEX,\n",
        "        history_size=HISTORY_SIZE,\n",
        "        target_size=TARGET_SIZE,\n",
        "    )\n",
        "\n",
        "    batch = 1\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\\\n",
        "        .shuffle(1440)\\\n",
        "        .batch(batch) \\\n",
        "        .cache()\\\n",
        "        # .repeat()\n",
        "\n",
        "    validate_dataset = tf.data.Dataset.from_tensor_slices((validate_x, validate_y))\\\n",
        "        .batch(batch)\\\n",
        "        # .repeat()\n",
        "\n",
        "    print(validate_y)\n",
        "\n",
        "    model = get_model(path=MODEL_TRAINED_PATH,\n",
        "                      input_shape=(1, 8),\n",
        "                      train=True)\n",
        "\n",
        "    model_history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=validate_dataset,\n",
        "        validation_steps=1440,\n",
        "        epochs=30,\n",
        "        # steps_per_epoch=19280,\n",
        "        # initial_epoch=10,\n",
        "        batch_size=1,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                filepath=MODEL_TRAINED_PATH,\n",
        "                save_weights_only=False,\n",
        "                save_best_only=True,\n",
        "            )\n",
        "        ],\n",
        "        use_multiprocessing=True,\n",
        "    )\n",
        "\n",
        "    loss = model_history.history['loss']\n",
        "    val_loss = model_history.history['val_loss']\n",
        "    epochs = range(len(loss))\n",
        "    plt.figure(1)\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import threading\n",
        "\n",
        "def set_interval(func, sec):\n",
        "    def func_wrapper():\n",
        "        set_interval(func, sec)\n",
        "        func()\n",
        "    t = threading.Timer(sec, func_wrapper)\n",
        "    t.start()\n",
        "    return t\n",
        "\n",
        "\n",
        "__cards = {\n",
        "    1: '1ï¸âƒ£',\n",
        "    2: '2ï¸âƒ£',\n",
        "    3: '3ï¸âƒ£',\n",
        "    4: '4ï¸âƒ£',\n",
        "    5: '5ï¸âƒ£',\n",
        "    6: '6ï¸âƒ£',\n",
        "    7: '7ï¸âƒ£',\n",
        "    8: '8ï¸âƒ£',\n",
        "    9: '9ï¸âƒ£',\n",
        "    10: 'ðŸ”Ÿ',\n",
        "    11: \"J\",\n",
        "    12: \"Q\",\n",
        "    13: \"K\",\n",
        "    14: \"A\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_card_by_id(id: float) -> str:\n",
        "    i = int(id)\n",
        "    return 'UNKNOWN' if i not in __cards else __cards[i]\n",
        "\n",
        "\n",
        "def card_to_vector(n: int) -> List[int]:\n",
        "    vec = [0] * 13\n",
        "    vec[n - 2] = 1\n",
        "    return vec\n",
        "\n",
        "\n",
        "def vector_to_card(vec) -> int:\n",
        "    return numpy.argmax(vec) + 2\n",
        "\n",
        "\n",
        "\n",
        "commands = {\n",
        "    'train': do_train,\n",
        "    'predict': do_predict,\n",
        "    'bot': do_bot,\n",
        "    'mass_test': do_mass_test,\n",
        "}\n",
        "\n",
        "\n",
        "do_mass_test(_)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-45f381eaff87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m \u001b[0mdo_mass_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-45f381eaff87>\u001b[0m in \u001b[0;36mdo_mass_test\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_TRAINED_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMASS_TEST_SINCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMASS_TEST_SINCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMASS_TEST_ITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-45f381eaff87>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(path, input_shape, train)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trained model on path {} not found'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Trained model on path trained not found"
          ]
        }
      ]
    }
  ]
}